{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "There is a real time fire dataset (Indoor_Fire_Dataset.csv) with the following description:\n",
        "\n",
        "\n",
        "The dataset contains 4 incipient fire scenarios (wood, candles, cable, lunts) along with 8 different nuisance scenarios (smoke from a fog machine, deodorant, ethanol, CO release, exhaust gases, green waste, welding, and abrasive cutting) carried out in a (10 x 22 x 8)m^3 industrial hall without ventilation. Each scenario was repeated 3 times in random with background sequences in between to reduce the influence of prehistory. The dataset consists of 248,502 rows and 18 columns and is structures as a continuous multivariate time series. Each row represents the sensor measurements (CO2, CO, H2, humidity, particulate matter of different sizes, air temperature, VOC and UV) from a unique sensor node position (\"Sensor_ID\") in the industrial hall at a specific timestamp. The columns correspond to the sensor measurements and include additional labels: a scenario-specific label (\"scenario_label\"), a binary label (\"anomaly_label\") distinguishing between \"Normal\" (background) and \"Anomaly\" (fire or nuisance scenario), a \"fire_label\" intersecting the anomalies into fire-relevant or non-fire-relevant anomalies and a progress label (\"progress_label\") that allows for dividing the events into sub-sequences based on ongoing physical sub-processes. The \"Sensor_ID\" column can be utilized to access data from different sensor node positions.\n",
        "\n"
      ],
      "metadata": {
        "id": "274xhzR6cF_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our database is different, for we need to transform this data into a new csv file in order to store that into our database. Instruction for each column:\n",
        "\n",
        "1. Temperature (float8) - same as the data column Temperature_Room\n",
        "2. Humidity (float8) - same as the data column Humidity_Room\n",
        "3. Pressure (float8) - keep every entry 1030.0.\n",
        "4. CO (float8) - same as the data column CO_Room\n",
        "5. CO2 (float8) - same as the data column CO2_Room\n",
        "6. Fire (bool) - if the \"anomaly_label\" is also not 0 AND the \"fire\" label is not 0, the entry is True. Else, the entry is false."
      ],
      "metadata": {
        "id": "4rY35Vogfxnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Indoor_Fire_Dataset.csv file\n",
        "df = pd.read_csv('Indoor_Fire_Dataset.csv')\n",
        "\n",
        "# Apply column transformations and additions\n",
        "df_transformed = pd.DataFrame({\n",
        "    'Temperature': df['Temperature_Room'],\n",
        "    'Humidity': df['Humidity_Room'],\n",
        "    'Pressure': 1030.0,\n",
        "    'CO': df['CO_Room'],\n",
        "    'CO2': df['CO2_Room'],\n",
        "    'Fire': (df['fire'] != 0.0) & (df['anomaly_label'] != 0.0)\n",
        "})\n",
        "\n",
        "# Save the transformed data to our_fire_data.csv\n",
        "df_transformed.to_csv('our_fire_data.csv', index=False)\n",
        "\n",
        "print(\"Data transformed and saved to 'our_fire_data.csv' successfully!\")"
      ],
      "metadata": {
        "id": "qNcLONVDfwIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b5c6d0-699c-4f31-dbd5-f4162f786c66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data transformed and saved to 'our_fire_data.csv' successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9MnExh31cOgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d251c6dc-5519-4d98-8eb2-07647037f611"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13c063ec"
      },
      "source": [
        "Here are some other supervised learning models commonly used for classification tasks, each with its own strengths and weaknesses:\n",
        "\n",
        "1.  **Decision Trees:**\n",
        "    *   **Concept:** Builds a tree-like model of decisions based on features to predict a target value. It splits data into subsets based on the value of input features.\n",
        "    *   **Pros:** Easy to understand and interpret (white-box model), can handle both numerical and categorical data, requires little data preparation.\n",
        "    *   **Cons:** Prone to overfitting, can be unstable (small variations in data might result in a completely different tree).\n",
        "\n",
        "2.  **Random Forest:**\n",
        "    *   **Concept:** An ensemble learning method that builds multiple decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "    *   **Pros:** Reduces overfitting compared to individual decision trees, generally more accurate and robust, can handle a large number of features.\n",
        "    *   **Cons:** Less interpretable than single decision trees, can be computationally expensive for very large datasets.\n",
        "\n",
        "3.  **Support Vector Machines (SVM):**\n",
        "    *   **Concept:** Finds the optimal hyperplane that best separates classes in a high-dimensional space. It aims to maximize the margin between the classes.\n",
        "    *   **Pros:** Effective in high-dimensional spaces, memory efficient (uses a subset of training points in the decision function), versatile due to different kernel functions.\n",
        "    *   **Cons:** Can be slow on large datasets, choosing the right kernel function and regularization parameters can be challenging.\n",
        "\n",
        "4.  **K-Nearest Neighbors (KNN):**\n",
        "    *   **Concept:** A non-parametric, lazy learning algorithm that classifies a data point based on the majority class of its 'k' nearest neighbors in the feature space.\n",
        "    *   **Pros:** Simple to understand and implement, no training phase (lazy learner), works well for multi-class problems.\n",
        "    *   **Cons:** Computationally expensive during prediction (has to calculate distance to all training examples), sensitive to irrelevant features and the scale of data, needs careful selection of 'k'.\n",
        "\n",
        "5.  **Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost):**\n",
        "    *   **Concept:** Builds an ensemble of weak prediction models (typically decision trees) sequentially. Each new tree corrects the errors of the previous ones.\n",
        "    *   **Pros:** Often provides state-of-the-art performance on many tabular datasets, handles various data types, robust to overfitting with proper tuning.\n",
        "    *   **Cons:** Can be more complex to tune than Random Forest, prone to overfitting if not tuned carefully, training can be time-consuming.\n",
        "\n",
        "6.  **Neural Networks (Deep Learning):**\n",
        "    *   **Concept:** Composed of layers of interconnected nodes (neurons), inspired by the human brain. They learn hierarchical representations of data.\n",
        "    *   **Pros:** Highly powerful for complex patterns, especially in image, text, and sequence data; can achieve very high accuracy with enough data and computational resources.\n",
        "    *   **Cons:** Requires very large datasets for optimal performance, computationally intensive, difficult to interpret (black-box model), prone to overfitting without proper regularization.\n",
        "\n",
        "The choice of model depends heavily on the specific problem, the nature of the data, the size of the dataset, computational resources, and the need for interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to build a predictive model using Sci-Kit learn library in Python. Linear classifier is probably a good model that helps us to process the data. First we need to classify the data \"our_fire_data.csv\" into training data and testing data, with each dataset having True and False outputs under column \"fire\". The training data should be saved as training.csv and the testing data should be saved as testing.csv. Just try one line to training and the next line to testing, then again the next line to training, and so on."
      ],
      "metadata": {
        "id": "t5P1NkH6sy2G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03c3ed76",
        "outputId": "22fa6a8a-e735-4b9c-828e-06d7bd56160b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the transformed data\n",
        "df_fire = pd.read_csv('our_fire_data.csv')\n",
        "\n",
        "# Split into training and testing sets based on alternating rows\n",
        "training_df = df_fire.iloc[::2]\n",
        "testing_df = df_fire.iloc[1::2]\n",
        "\n",
        "# Save the training and testing sets to new CSV files\n",
        "training_df.to_csv('training.csv', index=False)\n",
        "testing_df.to_csv('testing.csv', index=False)\n",
        "\n",
        "print(\"Data successfully split into 'training.csv' and 'testing.csv'!\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully split into 'training.csv' and 'testing.csv'!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then import the linear classification tools from Scikit-learn. These tools will be used on the five parameters: Temperature, Humidity, Pressure, CO, and CO2. The boolean value fire will be the result to be predicted. Write the import lines."
      ],
      "metadata": {
        "id": "4eDBUgyoHnT3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "685f551b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Part:\n",
        "Using the training.csv data, train the model to so that it would use linear classification to predict the fire output from the parameters."
      ],
      "metadata": {
        "id": "yK9OwAitJqw_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9fa8fa6",
        "outputId": "fec34fb6-112f-4b3f-9441-0bac0c09d2a3"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the training data\n",
        "training_df = pd.read_csv('training.csv')\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X_train = training_df[['Temperature', 'Humidity', 'Pressure', 'CO', 'CO2']]\n",
        "y_train = training_df['Fire']\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model trained successfully!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing part\n",
        "Find out the accuracy in percentage by applying testing.csv to the model."
      ],
      "metadata": {
        "id": "63wd7XbtKVc7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac33cb43",
        "outputId": "fab5087f-2062-4c72-ea0e-7e9e259ba0de"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the testing data\n",
        "testing_df = pd.read_csv('testing.csv')\n",
        "\n",
        "# Define features (X) and true target (y) for testing\n",
        "X_test = testing_df[['Temperature', 'Humidity', 'Pressure', 'CO', 'CO2']]\n",
        "y_test = testing_df['Fire']\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy_percentage = accuracy * 100\n",
        "\n",
        "print(f\"Model accuracy on the testing data: {accuracy_percentage:.2f}%\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy on the testing data: 88.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross validation\n",
        "Separate the data into training and testing data in a different way than above, then perform training and testing again."
      ],
      "metadata": {
        "id": "71Y0z8mHYucg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "342959f4",
        "outputId": "26c1e795-3da5-4b9a-e45e-5622bb59dd59"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the original transformed data\n",
        "df_fire = pd.read_csv('our_fire_data.csv')\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_fire[['Temperature', 'Humidity', 'Pressure', 'CO', 'CO2']]\n",
        "y = df_fire['Fire']\n",
        "\n",
        "# Split data into training and testing sets using a different method (random split)\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Data split into new training and testing sets successfully!\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into new training and testing sets successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aacd874"
      },
      "source": [
        "### Training the model with the new split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a15b4e",
        "outputId": "bc11a267-4de2-47b9-b88a-41ceb35432ef"
      },
      "source": [
        "# Initialize a new Logistic Regression model\n",
        "model_new = LogisticRegression()\n",
        "\n",
        "# Train the new model\n",
        "model_new.fit(X_train_new, y_train_new)\n",
        "\n",
        "print(\"New model trained successfully!\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New model trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4bd8c77"
      },
      "source": [
        "### Testing the model with the new split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89f618eb",
        "outputId": "db63d26e-c54b-4948-b56b-021166b50bd4"
      },
      "source": [
        "# Make predictions on the new test set\n",
        "y_pred_new = model_new.predict(X_test_new)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_new = accuracy_score(y_test_new, y_pred_new)\n",
        "accuracy_percentage_new = accuracy_new * 100\n",
        "\n",
        "print(f\"New model accuracy on the testing data: {accuracy_percentage_new:.2f}%\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New model accuracy on the testing data: 88.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilizing the model\n",
        "Now we have an input:\n",
        "\n",
        "Temperature = 30.60\n",
        "Humidity = 30.20\n",
        "Pressure = 1030.30\n",
        "CO = 3.10\n",
        "CO2 = 512.00\n",
        "\n",
        "Use the model to make a prediction."
      ],
      "metadata": {
        "id": "5KiI8_uUlZSS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "329990d8",
        "outputId": "d6979ebb-7bb6-410a-c4dc-14079372fb1c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the input values\n",
        "input_data = {\n",
        "    'Temperature': [30.60],\n",
        "    'Humidity': [30.20],\n",
        "    'Pressure': [1030.30],\n",
        "    'CO': [3.10],\n",
        "    'CO2': [512.00]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the input data\n",
        "input_df = pd.DataFrame(input_data)\n",
        "\n",
        "# Make a prediction using the trained model (model_new from cross-validation)\n",
        "prediction = model_new.predict(input_df)\n",
        "\n",
        "# Output the prediction\n",
        "if prediction[0]:\n",
        "    print(\"Prediction: Fire detected!\")\n",
        "else:\n",
        "    print(\"Prediction: No fire detected.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: No fire detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Model"
      ],
      "metadata": {
        "id": "fyRhrAwMcNtc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abcd5454",
        "outputId": "a12d89a3-6e23-486b-cd1a-2eabfa4d0c47"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the training and testing data (if not already loaded in the environment)\n",
        "# Re-loading to ensure availability in case the kernel state resets or for independent execution\n",
        "df_fire = pd.read_csv('our_fire_data.csv')\n",
        "X = df_fire[['Temperature', 'Humidity', 'Pressure', 'CO', 'CO2']]\n",
        "y = df_fire['Fire']\n",
        "\n",
        "# Split data into training and testing sets using a random split (as done previously for 'new' datasets)\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize the KNN classifier. A common choice for n_neighbors is 5.\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the KNN model\n",
        "knn_model.fit(X_train_new, y_train_new)\n",
        "\n",
        "print(\"KNN model trained successfully!\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_knn = knn_model.predict(X_test_new)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_knn = accuracy_score(y_test_new, y_pred_knn)\n",
        "accuracy_percentage_knn = accuracy_knn * 100\n",
        "\n",
        "print(f\"KNN model accuracy on the testing data: {accuracy_percentage_knn:.2f}%\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN model trained successfully!\n",
            "KNN model accuracy on the testing data: 92.48%\n"
          ]
        }
      ]
    }
  ]
}